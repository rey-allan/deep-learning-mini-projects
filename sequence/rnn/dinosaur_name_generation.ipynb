{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dinosaur Name Generation\n",
    "\n",
    "Generating dinosaur names using a character-level language model with RNNs.\n",
    "\n",
    "_PyTorch implementation of the assignment of Course 5 of Coursera's Deep Learning Specialization_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 24\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Device Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "We will create a custom dataset that will provide the following:\n",
    "\n",
    "- Each sample is each **character** of a dinosaur name\n",
    "- Each corresponding target is a **character** of the same name, shifted one to the right\n",
    "  - We shift the characters because we want to predict the next most likely character given some previous character\n",
    "- 1536 training examples\n",
    "- 19909 total characters\n",
    "- 27 unique characters in our vocabulary (a-z and the terminal `\\n` character)\n",
    "\n",
    "We will also define two mappings to track character-to-index and index-to-character values of our vocabulary. This will help us build the training set (using indices) and sample new names (using characters).\n",
    "\n",
    "_Note: We won't be using a `DataLoader` because it requires all samples to be the same size. In this case, the dinosaur names are of variable length. There are ways of padding and packing the samples so that they are all of the same size (see: https://towardsdatascience.com/taming-lstms-variable-sized-mini-batches-and-why-pytorch-is-good-for-your-health-61d35642972e). However, the size of this dataset is small enough to train efficiently without batches._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 27\n",
    "\n",
    "data = open('data/dinos.txt', 'r').read()\n",
    "data = data.lower()\n",
    "chars = sorted(list(set(data)))\n",
    "\n",
    "char_to_ix = {ch: i for i, ch in enumerate(chars)}\n",
    "ix_to_char = {i: ch for i, ch in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/dinos.txt', 'r') as f:\n",
    "    names = [n.lower().strip() for n in f.readlines()]\n",
    "\n",
    "# Generate the training examples with the indices for each character of each name\n",
    "samples = [[char_to_ix[ch] for ch in name] for name in names]\n",
    "# Generate the targets by shifting the values (i.e. start from index 1 instread of 0) and adding the terminal character\n",
    "targets = [sample[1:] + [char_to_ix['\\n']] for sample in samples]\n",
    "# Zip them into a single list of sample-target pairs\n",
    "training = list(zip(samples, targets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "We'll use a vanilla Recurrent Neural Network with a **single** recurrent layer with **50 units**, and a **softmax** output function at _each_ RNN cell. This means our model expects $T_x = T_y$, i.e. the length of the input sequence should match the length of the target sequence. The reason is that we are generating a character-level model using the _same_ name as both input and target (shifted).\n",
    "\n",
    "For the input, the model will one-hot encode the character indices based on the size of our vocabulary (28 characters).\n",
    "\n",
    "![](img/rnn.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        # RNN layer expects one-hot encoded sequences of size `vocab_size`\n",
    "        self.rnn = nn.RNN(input_size=vocab_size, hidden_size=50, batch_first=True)\n",
    "        # Output layer to convert to target space (`vocab_size` character classes)\n",
    "        self.out = nn.Linear(in_features=50, out_features=vocab_size)\n",
    "        # We don't define a softmax output layer explicitly because it's combined with the loss function\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        # One-hot the input\n",
    "        output = F.one_hot(input, num_classes=vocab_size).float()\n",
    "        # Pass through RNN along with the previous hidden activations\n",
    "        # Passing in the previous hidden activations is needed for the model to continue learning\n",
    "        # if they aren't passed, the activations are reset to zero which effectively erases any learnings from the model!\n",
    "        output, hidden = self.rnn(output, hidden)\n",
    "        # Stack outputs from RNN (i.e. batch_size * seq_len, hidden_size) and convert to target space\n",
    "        # In this case batch_size=1\n",
    "        output = self.out(output.contiguous().view(-1, 50))\n",
    "\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(\n",
      "  (rnn): RNN(27, 50, batch_first=True)\n",
      "  (out): Linear(in_features=50, out_features=27, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = Model()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling\n",
    "\n",
    "In order to generate dinosaur names, we will sample characters from our model according to their softmax distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Max number of characters in a name\n",
    "# As mentioned in the Deep Learning Specialization assignment, this should be unlikely with a well-trained model but it's a safety measure\n",
    "max_chars = 50\n",
    "\n",
    "def sample_name(model):\n",
    "    # Initialize the input as a sample with value zero of size (batch_size=1, seq_len=1)\n",
    "    input = torch.zeros([1, 1], dtype=torch.int64)\n",
    "    # The hidden activations are initialize to all zeroes by default when `None` is passed\n",
    "    hidden = None\n",
    "\n",
    "    name = \"\"\n",
    "    char = ''\n",
    "    chars = 0\n",
    "\n",
    "    # Make sure to not record any computation in the graph since the model is being used for inference here!\n",
    "    with torch.no_grad():\n",
    "        while char != '\\n' and chars < max_chars:\n",
    "            output, hidden = model(input.to(device), hidden)\n",
    "            output = output.detach().cpu()\n",
    "\n",
    "            # Compute the distribution using the softmax function\n",
    "            probs = F.softmax(output, dim=1)\n",
    "            # We squeeze the output to effectively convert it into a vector\n",
    "            probs = probs.numpy().squeeze()\n",
    "            \n",
    "            # Sample the character based on its distribution\n",
    "            # We normalize the probabilities by their sum to make sure they always sum to 1; this is needed because NumPy is very strict\n",
    "            idx = np.random.choice(list(range(vocab_size)), p=probs / probs.sum())\n",
    "\n",
    "            # Save the actual character and continue building the name\n",
    "            char = ix_to_char[int(idx)]\n",
    "            name += char\n",
    "\n",
    "            # The sampled character becomes the input to the next timestep\n",
    "            # Make sure to add the batch_size=1 and seq_len=1 dimensions\n",
    "            input = torch.tensor(idx).unsqueeze(0).unsqueeze(1)\n",
    "\n",
    "            chars += 1\n",
    "\n",
    "    return name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization\n",
    "\n",
    "We'll use Adam for training the model with the cross-entropy loss. As in the Deep Learning Specialization assignment, we will also smooth the loss to accelerate training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn =  nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth(prev_loss, loss):\n",
    "    # Taken from `utils.py` as part of the Deep Learning Specialization assignment\n",
    "    return prev_loss * 0.999 + loss * 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Time to train the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "epochs = 35000\n",
    "dino_names = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/35000]\tLoss: 23.051048278808594\n",
      "Tikcuyz\n",
      "Mqfcdstrhf\n",
      "Lqrolicxvcviujnivndauiutsknmjjuqqvbksr\n",
      "Nyijubjlwqmwrp\n",
      "Muzrsqucupsnqnscuesmxpdgxncufnscaevrdae\n",
      "Ljhdugt\n",
      "Uwqvpwsefitqnlcyjizofdhb\n",
      "\n",
      "\n",
      "[2000/35000]\tLoss: 4.896327972412109\n",
      "Esaulosonnablochives\n",
      "Imoputpolnlntonmanoblptysaurus\n",
      "Ilnholvalus\n",
      "Llldoththtingphus\n",
      "Osbunposan\n",
      "Llenglis\n",
      "Saropthalmus\n",
      "\n",
      "\n",
      "[4000/35000]\tLoss: 2.39034104347229\n",
      "Longlkioatopalopawaun\n",
      "Aoris\n",
      "Oon\n",
      "Torngugiauditapavocltitarlgocopapr\n",
      "Oduwx\n",
      "Awapelpolix\n",
      "Anjos\n",
      "\n",
      "\n",
      "[6000/35000]\tLoss: 2.0113978385925293\n",
      "Velaeor\n",
      "Ipcrutrosannulbantisaurus\n",
      "Xintan\n",
      "Odosatodledous\n",
      "Aonysaurus\n",
      "Ocodun\n",
      "Elodosaurosaus\n",
      "\n",
      "\n",
      "[8000/35000]\tLoss: 1.9952290058135986\n",
      "Ostliausmodmosaenzisaurus\n",
      "Elkostus\n",
      "Odadiestamas\n",
      "Orgag\n",
      "Amhofophrargas\n",
      "Osakanescenasanhuseutoshus\n",
      "Utor\n",
      "\n",
      "\n",
      "[10000/35000]\tLoss: 2.0411453247070312\n",
      "A\n",
      "Eiauloa\n",
      "Eatxpurosyusjirateryopaurus\n",
      "Ylutabagochylesaurus\n",
      "Aunthhaavocervurosaurus\n",
      "Alovajeelos\n",
      "Imodopsymachusaurus\n",
      "\n",
      "\n",
      "[12000/35000]\tLoss: 2.0199317932128906\n",
      "Aus\n",
      "Ys\n",
      "Ators\n",
      "Uabrrkuukaqus\n",
      "Aule\n",
      "Aus\n",
      "Overocryus\n",
      "\n",
      "\n",
      "[14000/35000]\tLoss: 1.9731882810592651\n",
      "Uerureps\n",
      "Anomalarralosaurus\n",
      "Atbesaurus\n",
      "Emuamptaspenasaurus\n",
      "Aurus\n",
      "Es\n",
      "Pamtatasaurus\n",
      "\n",
      "\n",
      "[16000/35000]\tLoss: 1.9964944124221802\n",
      "Uasrtasaurus\n",
      "Is\n",
      "Ala\n",
      "Anuwoauroitotan\n",
      "Ubaca\n",
      "Us\n",
      "Ari\n",
      "\n",
      "\n",
      "[18000/35000]\tLoss: 1.992660641670227\n",
      "Podrnunolen\n",
      "Eunas\n",
      "Uh\n",
      "Imonmanihidangllede\n",
      "Ichisaurus\n",
      "Andosaurus\n",
      "Atitoldsaudunnudirmautrronglidoshueldruruusaurus\n",
      "\n",
      "\n",
      "[20000/35000]\tLoss: 1.9938956499099731\n",
      "Teestegosaurus\n",
      "Manganongonvankeonooventenasaunananna\n",
      "Alatosaurus\n",
      "Ukya\n",
      "Os\n",
      "Asaurus\n",
      "Occytadon\n",
      "\n",
      "\n",
      "[22000/35000]\tLoss: 2.019714832305908\n",
      "Opepalilusaurus\n",
      "Cerdysaurus\n",
      "Ielgutosaokrlos\n",
      "Isaurus\n",
      "Olalirolowryorangkesaurus\n",
      "Miluros\n",
      "Amephulys\n",
      "\n",
      "\n",
      "[24000/35000]\tLoss: 2.003389358520508\n",
      "Lotlotsanhi\n",
      "Eoa\n",
      "Ochomalpatops\n",
      "Opautomicops\n",
      "Elopagon\n",
      "Agus\n",
      "A\n",
      "\n",
      "\n",
      "[26000/35000]\tLoss: 1.9877848625183105\n",
      "Anglonhagnatoves\n",
      "Loranin\n",
      "Anutor\n",
      "Olia\n",
      "Tator\n",
      "Ang\n",
      "On\n",
      "\n",
      "\n",
      "[28000/35000]\tLoss: 1.9977226257324219\n",
      "Lonosanidor\n",
      "Olydon\n",
      "Lus\n",
      "Ohin\n",
      "Alielatosaurosaurospteseemus\n",
      "Bassanalelabs\n",
      "Besaurus\n",
      "\n",
      "\n",
      "[30000/35000]\tLoss: 2.0058932304382324\n",
      "Alrontkltawatakaurus\n",
      "Egl\n",
      "Rosptorseasoa\n",
      "An\n",
      "Sauruparoengawathuranogprui\n",
      "Ran\n",
      "Atialedomedyry\n",
      "\n",
      "\n",
      "[32000/35000]\tLoss: 2.009381055831909\n",
      "Arasaura\n",
      "Eptor\n",
      "Aurus\n",
      "Ong\n",
      "Or\n",
      "Antaurus\n",
      "Aurus\n",
      "\n",
      "\n",
      "[34000/35000]\tLoss: 1.9784616231918335\n",
      "Alatrallakuplokosaurus\n",
      "Aratops\n",
      "Antaattoanles\n",
      "Algtiana\n",
      "Sargatepaurus\n",
      "Alathiltatiorus\n",
      "S\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Main training loop\n",
    "model = model.to(device)\n",
    "# Make sure to shuffle the examples\n",
    "np.random.shuffle(training)\n",
    "# Initialize loss as in the Deep Learning Specialization assignment\n",
    "prev_loss = -np.log(1. / vocab_size) * dino_names\n",
    "losses = []\n",
    "# The hidden state is initialized to zeroes when `None` is passed\n",
    "hidden = None\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Grab an example\n",
    "    sample, target = training[epoch % len(training)]\n",
    "    # Add the batch_size=1 dimension\n",
    "    sample = torch.tensor(sample).unsqueeze(0).to(device)\n",
    "    target = torch.tensor(target).to(device)\n",
    "    \n",
    "    output, hidden = model(sample, hidden)\n",
    "\n",
    "    loss = loss_fn(output, target)\n",
    "    loss = smooth(prev_loss, loss)\n",
    "    \n",
    "    loss.backward()\n",
    "\n",
    "    # Clip gradients using a max value of 5 as in the Deep Learning Specialization assignment\n",
    "    nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
    "\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Detach the loss to avoid saving more computations on it since we'll only use it for smoothing the next loss\n",
    "    prev_loss = loss.detach().cpu().numpy()\n",
    "    # Detach the hidden activations to avoid saving more computations on it since we'll only use it for initializing the next RNN sequence\n",
    "    # If we don't detach PyTorch tries to backprop through all the history of hidden activations which isn't allowed since the resources are freed\n",
    "    hidden = hidden.detach()\n",
    "    \n",
    "    if epoch % 2000 == 0:\n",
    "        print(f'[{epoch}/{epochs}]\\tLoss: {loss}')\n",
    "        losses.append(loss)\n",
    "\n",
    "        # Generate some dinosaur names to see if the model is learning\n",
    "        for _ in range(dino_names):\n",
    "            name = sample_name(model)\n",
    "            print(f'{name[0].upper() + name[1:]}', end='')\n",
    "\n",
    "        print('\\n')\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), 'model/model.pt')\n",
    "\n",
    "# Plot the losses\n",
    "plt.plot(np.squeeze(losses))\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epochs (per two-thousands)')\n",
    "plt.title('Loss per Epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "Now that we have trained our model, we can use it to sample new dinosaur names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Asaurotreus\n",
      "Osia\n",
      "Olodosaus\n",
      "Oangesaurus\n",
      "Epia\n",
      "Eponator\n",
      "Epurontaurus\n",
      "Nochacrusus\n",
      "Osaurus\n",
      "Anosauptor\n"
     ]
    }
   ],
   "source": [
    "# Load the trained model\n",
    "model = Model()\n",
    "model.load_state_dict(torch.load('model/model.pt'))\n",
    "# Always set the mode to `eval` for inference\n",
    "model.eval()\n",
    "model = model.to(device)\n",
    "\n",
    "# Names to generate\n",
    "num_names = 10\n",
    "# The generated dinosaur names\n",
    "names = []\n",
    "\n",
    "for _ in range(num_names):\n",
    "    name = sample_name(model)\n",
    "    names.append(name)\n",
    "\n",
    "# Display all dinosaur names capitalized\n",
    "for name in names:\n",
    "    print(f'{name[0].upper() + name[1:]}', end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model was able to learn that dinosaur names tend to end in _saurus_, _don_, _aura_, _tor_, etc!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
