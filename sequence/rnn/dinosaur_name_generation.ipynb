{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dinosaur Name Generation\n",
    "\n",
    "Generating dinosaur names using a character-level language model with RNNs.\n",
    "\n",
    "_PyTorch implementation of the assignment of Course 5 of Coursera's Deep Learning Specialization_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 24\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Device Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "We will create a custom dataset that will provide the following:\n",
    "\n",
    "- Each sample is each **character** of a dinosaur name\n",
    "- Each corresponding target is a **character** of the same name, shifted one to the right\n",
    "  - We shift the characters because we want to predict the next most likely character given some previous character\n",
    "- 1536 training examples\n",
    "- 19909 total characters\n",
    "- 28 unique characters in our vocabulary (a-z, the terminal `\\n` character and a special `<PAD>` character)\n",
    "\n",
    "We will also define two mappings to track character-to-index and index-to-character values of our vocabulary. This will help us build the training set (using indices) and sample new names (using characters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = open('data/dinos.txt', 'r').read()\n",
    "data = data.lower()\n",
    "chars = sorted(list(set(data)))\n",
    "# Add the <PAD> character\n",
    "chars.append('<PAD>')\n",
    "\n",
    "char_to_ix = {ch: i for i, ch in enumerate(chars)}\n",
    "ix_to_char = {i: ch for i, ch in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DinosaurNamesDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        with open('data/dinos.txt', 'r') as f:\n",
    "            self.dataset = [x.lower().strip() for x in f.readlines()]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        # Generate the training examples with the indices for each character\n",
    "        X = [char_to_ix[ch] for ch in self.dataset[idx]]\n",
    "        # Generate the target by shifting the values (i.e. start from index 1 instead of 0) and adding the terminal\n",
    "        y = X[1:] + [char_to_ix['\\n']]\n",
    "\n",
    "        return torch.tensor(X), torch.tensor(y)\n",
    "\n",
    "\n",
    "def pad(batch):\n",
    "    # Pads the sequences in a batch so they can all have the same length and thus be used in a `DataLoader`\n",
    "    # Based on https://stackoverflow.com/a/57207895\n",
    "    # Ref 1: https://towardsdatascience.com/taming-lstms-variable-sized-mini-batches-and-why-pytorch-is-good-for-your-health-61d35642972e\n",
    "    # Ref 2: https://pytorch.org/tutorials/beginner/chatbot_tutorial.html#prepare-data-for-models\n",
    "    X = [x for (x, _) in batch]\n",
    "    y = [y for (_, y) in batch]\n",
    "    \n",
    "    # Compute the lengths of the samples' sequences before padding, needed for packing them all later\n",
    "    X_lengths = [x.shape[0] for (x, _) in batch]\n",
    "\n",
    "    # Pad sequences to make them the same length\n",
    "    pad_value = char_to_ix['<PAD>']\n",
    "    X = rnn_utils.pad_sequence(X, batch_first=True, padding_value=pad_value)\n",
    "    y = rnn_utils.pad_sequence(y, batch_first=True, padding_value=pad_value)\n",
    "    \n",
    "    # Compute the masks for the targets, needed for masking out the loss computation later\n",
    "    mask = y != pad_value\n",
    "\n",
    "    return X, y, X_lengths, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create our training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The `collate_fn` applies a transformation to the batch\n",
    "training = DataLoader(DinosaurNamesDataset(), batch_size=64, shuffle=True, num_workers=4, collate_fn=pad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "We'll use a vanilla Recurrent Neural Network with a **single** recurrent layer with **50 units**, and a **softmax** output function at _each_ RNN cell. This means our model expects $T_x = T_y$, i.e. the length of the input sequence should match the length of the target sequence. The reason is that we are generating a character-level model using the _same_ name as both input and target (shifted).\n",
    "\n",
    "For the input, the model will one-hot encode the character indices based on the size of our vocabulary (28 characters).\n",
    "\n",
    "![](img/rnn.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        self.rnn = nn.RNN(input_size=28, hidden_size=50, batch_first=True)\n",
    "        # Output layer to convert to target space (28 character classes)\n",
    "        self.out = nn.Linear(in_features=50, out_features=28)\n",
    "\n",
    "    def forward(self, X, X_lengths):\n",
    "        # One-hot the input\n",
    "        output = F.one_hot(X, num_classes=28).float()\n",
    "\n",
    "        # `pack_padded_sequence` so that padded items in the sequence won't be shown to the RNN\n",
    "        # See: https://towardsdatascience.com/taming-lstms-variable-sized-mini-batches-and-why-pytorch-is-good-for-your-health-61d35642972e\n",
    "        output = rnn_utils.pack_padded_sequence(output, X_lengths, batch_first=True, enforce_sorted=False)\n",
    "\n",
    "        # Pass through RNN\n",
    "        output, _ = self.rnn(output)\n",
    "\n",
    "        # Undo the packing operation\n",
    "        # See: https://towardsdatascience.com/taming-lstms-variable-sized-mini-batches-and-why-pytorch-is-good-for-your-health-61d35642972e\n",
    "        output, _ = rnn_utils.pad_packed_sequence(output, batch_first=True)\n",
    "\n",
    "        # Convert to target space\n",
    "        output = self.out(output)\n",
    "\n",
    "        # Apply softmax on the vocab dimension explicitly (as opposed to using PyTorch's combined loss function) since we will be defining\n",
    "        # our own custom loss function\n",
    "        output = F.softmax(output, dim=2)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(\n",
      "  (rnn): RNN(28, 50, batch_first=True)\n",
      "  (out): Linear(in_features=50, out_features=28, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = Model()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization\n",
    "\n",
    "We'll use Adam for training the model with a custom cross-entropy loss. A custom loss function is needed because we don't want to consider the elements that correspond to the special character `<PAD>` in the computation.\n",
    "\n",
    "_Note: see https://pytorch.org/tutorials/beginner/chatbot_tutorial.html#masked-loss_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_cross_entropy_loss(output, targets, mask):\n",
    "    # `torch.gather()` grabs the output activations that correspond to the \"indexes\" given by the targets\n",
    "    # e.g. if the target is the character with index 5, then it grabs the activation at index 5\n",
    "    # Once we have the activations that correspond to the targets, we can compute the negative log likelihood, i.e. the cross-entropy\n",
    "    # This is equivalent to having the targets be vectors of 0's with a 1 in the corresponding index which would zero out all other computations\n",
    "    cross_entropy = -torch.log(torch.gather(output.view(-1, 28), 1, targets.view(-1, 1)).squeeze(1))\n",
    "    # Now mask out the padded values and reduce the losses using the average to generate the final loss for the batch\n",
    "    loss = cross_entropy.masked_select(mask.view(-1, 1)).mean()\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = masked_cross_entropy_loss\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Time to train the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "epochs = 35000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/35000]\tLoss: 3.1735522747039795\n",
      "[1000/35000]\tLoss: 0.7182769775390625\n",
      "[2000/35000]\tLoss: 0.6934554576873779\n",
      "[3000/35000]\tLoss: 0.683504581451416\n",
      "[4000/35000]\tLoss: 0.6608338952064514\n",
      "[5000/35000]\tLoss: 0.7030109763145447\n",
      "[6000/35000]\tLoss: 0.6749889254570007\n",
      "[7000/35000]\tLoss: 0.6569188237190247\n",
      "[8000/35000]\tLoss: 0.6603567600250244\n",
      "[9000/35000]\tLoss: 0.679055392742157\n",
      "[10000/35000]\tLoss: 0.655177652835846\n",
      "[11000/35000]\tLoss: 0.6694705486297607\n",
      "[12000/35000]\tLoss: 0.7158105969429016\n",
      "[13000/35000]\tLoss: 0.680025041103363\n",
      "[14000/35000]\tLoss: 0.6461178660392761\n",
      "[15000/35000]\tLoss: 0.6865223050117493\n",
      "[16000/35000]\tLoss: 0.6566489338874817\n",
      "[17000/35000]\tLoss: 0.6718356609344482\n",
      "[18000/35000]\tLoss: 0.6508365869522095\n",
      "[19000/35000]\tLoss: 0.6647751331329346\n",
      "[20000/35000]\tLoss: 0.6493016481399536\n",
      "[21000/35000]\tLoss: 0.6633304357528687\n",
      "[22000/35000]\tLoss: 0.6532465815544128\n",
      "[23000/35000]\tLoss: 0.6576023101806641\n",
      "[24000/35000]\tLoss: 0.6657090783119202\n",
      "[25000/35000]\tLoss: 0.6511833667755127\n",
      "[26000/35000]\tLoss: 0.6599805951118469\n",
      "[27000/35000]\tLoss: 0.6529178023338318\n",
      "[28000/35000]\tLoss: 0.6455410718917847\n",
      "[29000/35000]\tLoss: 0.6556876301765442\n",
      "[30000/35000]\tLoss: 0.6552203297615051\n",
      "[31000/35000]\tLoss: 0.6572830677032471\n",
      "[32000/35000]\tLoss: 0.6773163676261902\n",
      "[33000/35000]\tLoss: 0.6764679551124573\n",
      "[34000/35000]\tLoss: 0.6476970314979553\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Main training loop\n",
    "model = model.to(device)\n",
    "total_losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    losses = []\n",
    "\n",
    "    for X, y, X_lengths, mask in training:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        mask = mask.to(device)\n",
    "\n",
    "        output = model(X, X_lengths)\n",
    "\n",
    "        loss = loss_fn(output, y, mask)\n",
    "\n",
    "        loss.backward()\n",
    "        \n",
    "        # Clip gradients using a max value of 5 as in the Deep Learning Specialization assignment\n",
    "        nn.utils.clip_grad_value_(model.parameters(), 5)\n",
    "\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Detach the loss to avoid saving any more computations on it\n",
    "        losses.append(loss.detach().cpu().numpy())\n",
    "\n",
    "    if epoch % 1000 == 0:\n",
    "        total_losses.append(np.mean(losses))\n",
    "        print(f'[{epoch}/{epochs}]\\tLoss: {total_losses[-1]}')\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), 'model/model.pt')\n",
    "\n",
    "# Plot the losses\n",
    "plt.plot(np.squeeze(total_losses))\n",
    "plt.ylabel('Avg. Loss')\n",
    "plt.xlabel('Epochs (per thousands)')\n",
    "plt.title('Avg. Loss per Epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "Now that we have trained our model, we can use it to sample new dinosaur names!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uchalillorucanorizeoraurelurutalisisthyulinabarlua\n",
      "Isuriseinaledanyaraneriamalungicheosaelatinaneuaus\n",
      "Amorusineomanisuchosinadrrunepravesteralalalinyali\n",
      "Utralicralisusheprisiscrhinovistadesceorulumanycov\n",
      "Olilantadrinquekeutanisileneonotanycaganeolaneuneo\n",
      "Ueucilepelalehamaneushorisyilaliledestyuhicojustra\n",
      "Unusiralalorunorigorugerisholinalutyailerucacharai\n",
      "Ereosinojigorilulilasanilautrhadinodeusiseuevauapo\n",
      "Oveunigwutorheraus\n",
      "\n",
      "Odeoriyumohyaceuririneyisushoshazeomanoligistautat\n"
     ]
    }
   ],
   "source": [
    "# Load the trained model\n",
    "model = Model()\n",
    "model.load_state_dict(torch.load('model/model.pt'))\n",
    "# Always set the mode to `eval` for inference\n",
    "model.eval()\n",
    "model = model.to(device)\n",
    "\n",
    "# Names to generate\n",
    "num_names = 10\n",
    "# Max number of characters in a name\n",
    "# As mentioned in the Deep Learning Specialization assignment, this should be unlikely with a well-trained model but it's a safety measure\n",
    "max_chars = 50\n",
    "# The generated dinosaur names\n",
    "names = []\n",
    "\n",
    "# Main sampling loop\n",
    "for _ in range(num_names):\n",
    "    # Initialize the input as a sampel with value zero of size (batch_size=1, seq_len=1, 28)\n",
    "    input = torch.tensor(0).unsqueeze(0).unsqueeze(1)\n",
    "    name = \"\"\n",
    "    char = ''\n",
    "    chars = 0\n",
    "\n",
    "    while char != '\\n' and chars < max_chars:\n",
    "        output = model(input.to(device), [1])\n",
    "        output = output.detach().cpu().flatten()\n",
    "\n",
    "        # We use PyTorch's multinomial vs NumPy's random choice because it handles probabilities that are very close to 1 (e.g. 1.000001) much better\n",
    "        # See: https://discuss.pytorch.org/t/torch-equivalent-of-numpy-random-choice/16146/14\n",
    "        idx = np.squeeze(output.multinomial(num_samples=1, replacement=True).numpy())\n",
    "\n",
    "        # Save the actual character and continue building the name\n",
    "        char = ix_to_char[int(idx)]\n",
    "        name += char\n",
    "\n",
    "        # The sampled character becomes the input to the next timestep\n",
    "        # Make sure to add the batch_size=1 and seq_len=1 dimensions\n",
    "        input = torch.tensor(idx).unsqueeze(0).unsqueeze(1)\n",
    "\n",
    "        chars += 1\n",
    "\n",
    "    names.append(name)\n",
    "\n",
    "# Display all dinosaur names capitalized\n",
    "for name in names:\n",
    "    print(f'{name[0].upper() + name[1:]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
